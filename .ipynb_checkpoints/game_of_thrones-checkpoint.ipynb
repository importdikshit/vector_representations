{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All the native dependencies. \n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All the non native dependencies.\n",
    "\n",
    "import nltk \n",
    "import gensim.models.word2vec as word\n",
    "import numpy as np\n",
    "import sklearn.manifold\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL THE SETUP HAS BEEN DONE! SOME PRELIMINARY DATA CLEANING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "book_names = os.listdir('./game_of_thrones/')\n",
    "book_names = sorted(glob.glob('./game_of_thrones/*.txt'))\n",
    "## Apparently, glob does proper filename expansion. Therefore glob > os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./game_of_thrones/got1.txt',\n",
       " './game_of_thrones/got2.txt',\n",
       " './game_of_thrones/got3.txt',\n",
       " './game_of_thrones/got4.txt',\n",
       " './game_of_thrones/got5.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for book_name in book_names:\n",
    "#     this_one = open(book_name)\n",
    "#     this_one.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COME BACK AND DO MORE EXTENSIVE DATA CLEANING TO SEE IF IT CHANGES RESULTS.\n",
    "\n",
    "### https://people.duke.edu/~ccc14/sta-663/TextProcessingExtras.html\n",
    "### https://www.analyticsvidhya.com/blog/2014/11/text-data-cleaning-steps-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting all books to UTF8 and putting them into the same string. \n",
    "# Interesting article on encodings - if you every need it again. \n",
    "# http://kunststube.net/encoding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is 1770659 characters long\n",
      "Corpus is 4071041 characters long\n",
      "Corpus is 6391405 characters long\n",
      "Corpus is 8107945 characters long\n",
      "Corpus is 9719485 characters long\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for this_book in book_names:\n",
    "    with codecs.open(this_book, \"r\", \"utf-8\") as book:\n",
    "        corpus_raw += book.read()\n",
    "    print(\"Corpus is {0} characters long\".format(len(corpus_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT TRY AND CALL THE CORPUS TO VIEW ITS CONTENTS. \n",
    "# Learnt this the hard way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This edition contains the complete text of the original hardcover edition.\\n\\nNOT ONE WORD HAS BEEN OMITTED.\\n\\nA CLASH OF KINGS\\n\\nA Bantam Spectra Book\\n\\nPUBLISHING HISTORY\\n\\nBantam Spectra hardcover edition published February 1999\\n\\nBantam Spectra paperback edition / September 2000\\n\\nSPECTRA and the portrayal of a boxed “s” are trademarks of Bantam Books, a division of Random House, Inc.\\n\\nAll rights reserved.\\n\\nCopyright © 1999 by George R. R. Martin.\\n\\nMaps by James Sinclair.\\n\\nHeraldic crest by Virginia Norey.\\n\\nLibrary of Congress Catalog Card Number: 98-37954.\\n\\nNo part of this book may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, recording, or by any information storage and retrieval system, without permission in writing from the publisher.\\n\\nVisit our website at www.bantamdell.com\\n\\nBantam Books, the rooster colophon, Spectra and the portrayal of a boxed “s” are registered trademarks of Random House Inc.\\n\\neISBN: 978-0-553-89785-2\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_raw[:1000]\n",
    "# ALL OF THIS JUNK NEEDS TO DISAPPEAR IN THE SECOND PASS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'was', 'here', 'the', 'ravens', 'came', 'after', 'long', 'flight']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sentences[10]\n",
    "\n",
    "# Split these tokenized sentences, down to tokenized words\n",
    "\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words\n",
    "\n",
    "raw_example = sentence_to_wordlist(raw_sentences[10])\n",
    "raw_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heraldic crest by Virginia Norey.\n",
      "['Heraldic', 'crest', 'by', 'Virginia', 'Norey']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[5])\n",
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 1,818,103 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I DO NOT KNOW WHY WE ARENT JUST SIMPLY TAKING THE WORDS AND CONCATENATING THEM. THERE SEEMS TO BE SOME REASON TO PRESERVE THE SENTENCE LEVEL INTEGRITY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS!\n",
    "\n",
    "num_features = 300\n",
    "min_word_count = 3\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "context_size = 7\n",
    "downsampling = 1e-3\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thrones = word.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thrones.build_vocab(sentences)\n",
    "\n",
    "# Main Step - starts training on the given corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-13 23:05:25,406 : INFO : training model with 4 workers on 17277 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=7\n",
      "2017-06-13 23:05:26,496 : INFO : PROGRESS: at 1.97% examples, 134782 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:27,511 : INFO : PROGRESS: at 4.41% examples, 150675 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:28,522 : INFO : PROGRESS: at 6.95% examples, 156604 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:29,560 : INFO : PROGRESS: at 9.54% examples, 160191 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:30,568 : INFO : PROGRESS: at 12.01% examples, 163473 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:31,569 : INFO : PROGRESS: at 14.42% examples, 164392 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:32,647 : INFO : PROGRESS: at 17.26% examples, 166558 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:33,671 : INFO : PROGRESS: at 19.80% examples, 169152 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:34,673 : INFO : PROGRESS: at 22.52% examples, 171656 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-13 23:05:35,751 : INFO : PROGRESS: at 25.07% examples, 170921 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:36,771 : INFO : PROGRESS: at 28.03% examples, 172605 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:37,883 : INFO : PROGRESS: at 30.38% examples, 170827 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:38,886 : INFO : PROGRESS: at 32.44% examples, 169028 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:39,991 : INFO : PROGRESS: at 34.65% examples, 166727 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:40,994 : INFO : PROGRESS: at 37.04% examples, 166376 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:42,006 : INFO : PROGRESS: at 38.68% examples, 163609 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-13 23:05:43,016 : INFO : PROGRESS: at 40.52% examples, 162090 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:44,030 : INFO : PROGRESS: at 42.98% examples, 162394 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:45,093 : INFO : PROGRESS: at 45.50% examples, 162634 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:46,141 : INFO : PROGRESS: at 48.39% examples, 163368 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:47,150 : INFO : PROGRESS: at 50.78% examples, 163925 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:48,158 : INFO : PROGRESS: at 53.22% examples, 164494 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:49,163 : INFO : PROGRESS: at 55.89% examples, 164968 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:50,307 : INFO : PROGRESS: at 58.36% examples, 164486 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-13 23:05:51,403 : INFO : PROGRESS: at 60.51% examples, 163763 words/s, in_qsize 6, out_qsize 1\n",
      "2017-06-13 23:05:52,579 : INFO : PROGRESS: at 62.73% examples, 162351 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:53,686 : INFO : PROGRESS: at 64.50% examples, 160353 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-13 23:05:54,705 : INFO : PROGRESS: at 66.20% examples, 158735 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:55,707 : INFO : PROGRESS: at 68.37% examples, 158072 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:56,753 : INFO : PROGRESS: at 70.77% examples, 158462 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:57,803 : INFO : PROGRESS: at 73.31% examples, 159059 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:58,830 : INFO : PROGRESS: at 76.27% examples, 159931 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:05:59,866 : INFO : PROGRESS: at 78.75% examples, 160466 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:06:00,876 : INFO : PROGRESS: at 81.16% examples, 160881 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:06:01,880 : INFO : PROGRESS: at 83.62% examples, 161105 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:06:02,907 : INFO : PROGRESS: at 86.21% examples, 161441 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-13 23:06:03,950 : INFO : PROGRESS: at 88.37% examples, 160677 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:06:04,992 : INFO : PROGRESS: at 90.46% examples, 160349 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:06:06,006 : INFO : PROGRESS: at 92.30% examples, 159592 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:06:07,036 : INFO : PROGRESS: at 94.26% examples, 158966 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-13 23:06:08,060 : INFO : PROGRESS: at 96.52% examples, 158591 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-13 23:06:09,151 : INFO : PROGRESS: at 98.56% examples, 158139 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-13 23:06:09,686 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-06-13 23:06:09,728 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-13 23:06:09,731 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-13 23:06:09,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-13 23:06:09,752 : INFO : training on 9090515 raw words (7019467 effective words) took 44.3s, 158497 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7019467"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thrones.train(sentences, total_examples= thrones.corpus_count, epochs= thrones.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-13 23:07:42,043 : INFO : saving Word2Vec object under ./trained_models/thrones.w2v, separately None\n",
      "2017-06-13 23:07:42,047 : INFO : not storing attribute syn0norm\n",
      "2017-06-13 23:07:42,048 : INFO : not storing attribute cum_table\n",
      "2017-06-13 23:07:43,616 : INFO : saved ./trained_models/thrones.w2v\n"
     ]
    }
   ],
   "source": [
    "thrones.save(os.path.join(\"./trained_models/\", \"thrones.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## DONE. TRAINED THIS MODEL - GAME OF THRONES. NOW, TRAINING HARRY POTTER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_names = os.listdir('./harry_potter/')\n",
    "book_names = sorted(glob.glob('./harry_potter/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./harry_potter/harry1.txt',\n",
       " './harry_potter/harry2.txt',\n",
       " './harry_potter/harry3.txt',\n",
       " './harry_potter/harry4.txt',\n",
       " './harry_potter/harry5.txt',\n",
       " './harry_potter/harry6.txt',\n",
       " './harry_potter/harry7.txt']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is 448810 characters long\n",
      "Corpus is 947729 characters long\n",
      "Corpus is 1569784 characters long\n",
      "Corpus is 2690176 characters long\n",
      "Corpus is 4190900 characters long\n",
      "Corpus is 5182402 characters long\n",
      "Corpus is 6321546 characters long\n"
     ]
    }
   ],
   "source": [
    "harry_raw = u\"\"\n",
    "\n",
    "for this_book in book_names:\n",
    "    with codecs.open(this_book, \"r\", \"utf-8\") as book:\n",
    "        harry_raw += book.read()\n",
    "    print(\"Corpus is {0} characters long\".format(len(harry_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR FUTURE REFERENCE - MAKE SURE EVERYTHING IS UTF8 GOD DAMN. \n",
    "\n",
    "## THERE IS AN ICONV COMMAND IN LINUX WHICH DIDNT WORK FOR SOME REASON\n",
    "\n",
    "## THERE ARE DRAG AND DROP WEBSITES\n",
    "\n",
    "## WORST CASE SCENARIO - DO IT MANUALLY BY SAVING FILES AS\n",
    "\n",
    "## LOOK FOR MORE IDEAS. LOOK FOR ALTERNATE SOURCES OF DATA?\n",
    "\n",
    "#### THIS WAS A GENUINE PAIN IN THE ASS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone \\r\\n\\r\\nCHAPTER ONE \\r\\n\\r\\nTHE BOY WHO LIVED \\r\\n\\r\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \\r\\n\\r\\nMr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. \\r\\n\\r\\nThe Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about t\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harry_raw[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "harry = tokenizer.tokenize(harry_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "harry_sentences = []\n",
    "\n",
    "for this_harry in harry:\n",
    "    if len(this_harry) > 0:\n",
    "        harry_sentences.append(sentence_to_wordlist(this_harry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Dursleys',\n",
       " 'had',\n",
       " 'a',\n",
       " 'small',\n",
       " 'son',\n",
       " 'called',\n",
       " 'Dudley',\n",
       " 'and',\n",
       " 'in',\n",
       " 'their',\n",
       " 'opinion',\n",
       " 'there',\n",
       " 'was',\n",
       " 'no',\n",
       " 'finer',\n",
       " 'boy',\n",
       " 'anywhere']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harry_sentences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 1,120,879 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in harry_sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "harry_model = word.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-14 00:11:11,041 : INFO : collecting all words and their counts\n",
      "2017-06-14 00:11:11,043 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-14 00:11:11,082 : INFO : PROGRESS: at sentence #10000, processed 127278 words, keeping 9075 word types\n",
      "2017-06-14 00:11:11,123 : INFO : PROGRESS: at sentence #20000, processed 255379 words, keeping 12893 word types\n",
      "2017-06-14 00:11:11,179 : INFO : PROGRESS: at sentence #30000, processed 374742 words, keeping 15694 word types\n",
      "2017-06-14 00:11:11,219 : INFO : PROGRESS: at sentence #40000, processed 482966 words, keeping 17534 word types\n",
      "2017-06-14 00:11:11,272 : INFO : PROGRESS: at sentence #50000, processed 633232 words, keeping 20076 word types\n",
      "2017-06-14 00:11:11,331 : INFO : PROGRESS: at sentence #60000, processed 785997 words, keeping 22117 word types\n",
      "2017-06-14 00:11:11,388 : INFO : PROGRESS: at sentence #70000, processed 914841 words, keeping 24013 word types\n",
      "2017-06-14 00:11:11,444 : INFO : PROGRESS: at sentence #80000, processed 1047388 words, keeping 25639 word types\n",
      "2017-06-14 00:11:11,470 : INFO : collected 26376 word types from a corpus of 1120879 raw words and 85780 sentences\n",
      "2017-06-14 00:11:11,472 : INFO : Loading a fresh vocabulary\n",
      "2017-06-14 00:11:11,548 : INFO : min_count=3 retains 13183 unique words (49% of original 26376, drops 13193)\n",
      "2017-06-14 00:11:11,549 : INFO : min_count=3 leaves 1104138 word corpus (98% of original 1120879, drops 16741)\n",
      "2017-06-14 00:11:11,612 : INFO : deleting the raw counts dictionary of 26376 items\n",
      "2017-06-14 00:11:11,615 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2017-06-14 00:11:11,617 : INFO : downsampling leaves estimated 848907 word corpus (76.9% of prior 1104138)\n",
      "2017-06-14 00:11:11,619 : INFO : estimated required memory for 13183 words and 300 dimensions: 38230700 bytes\n",
      "2017-06-14 00:11:11,695 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "harry_model.build_vocab(harry_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-14 00:11:48,040 : INFO : training model with 4 workers on 13183 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=7\n",
      "2017-06-14 00:11:49,071 : INFO : PROGRESS: at 3.63% examples, 146691 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:11:50,112 : INFO : PROGRESS: at 8.49% examples, 167223 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-14 00:11:51,121 : INFO : PROGRESS: at 13.36% examples, 172831 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-14 00:11:52,155 : INFO : PROGRESS: at 18.37% examples, 180290 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:11:53,184 : INFO : PROGRESS: at 22.40% examples, 175028 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:11:54,184 : INFO : PROGRESS: at 26.89% examples, 175467 words/s, in_qsize 6, out_qsize 1\n",
      "2017-06-14 00:11:55,226 : INFO : PROGRESS: at 31.00% examples, 175001 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:11:56,244 : INFO : PROGRESS: at 35.82% examples, 176976 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:11:57,253 : INFO : PROGRESS: at 41.21% examples, 180298 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:11:58,264 : INFO : PROGRESS: at 45.96% examples, 181312 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:11:59,265 : INFO : PROGRESS: at 50.97% examples, 183464 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:00,288 : INFO : PROGRESS: at 56.29% examples, 184846 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:01,311 : INFO : PROGRESS: at 61.03% examples, 186132 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:02,336 : INFO : PROGRESS: at 65.05% examples, 184095 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:03,381 : INFO : PROGRESS: at 69.08% examples, 182013 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-14 00:12:04,416 : INFO : PROGRESS: at 74.54% examples, 183471 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:05,432 : INFO : PROGRESS: at 79.40% examples, 184402 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:06,443 : INFO : PROGRESS: at 84.73% examples, 185379 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:07,455 : INFO : PROGRESS: at 89.34% examples, 185891 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:08,469 : INFO : PROGRESS: at 94.40% examples, 186841 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:09,498 : INFO : PROGRESS: at 99.28% examples, 186928 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:10,532 : INFO : PROGRESS: at 104.57% examples, 187483 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:11,575 : INFO : PROGRESS: at 109.61% examples, 188084 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:12,605 : INFO : PROGRESS: at 114.77% examples, 188240 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:13,617 : INFO : PROGRESS: at 118.47% examples, 186944 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:14,697 : INFO : PROGRESS: at 122.40% examples, 185614 words/s, in_qsize 7, out_qsize 1\n",
      "2017-06-14 00:12:15,745 : INFO : PROGRESS: at 127.43% examples, 185856 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:16,754 : INFO : PROGRESS: at 132.00% examples, 185428 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:17,836 : INFO : PROGRESS: at 136.05% examples, 184248 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:18,887 : INFO : PROGRESS: at 139.47% examples, 182577 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:19,913 : INFO : PROGRESS: at 143.89% examples, 182066 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-14 00:12:20,931 : INFO : PROGRESS: at 147.46% examples, 180824 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-14 00:12:21,531 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-06-14 00:12:21,573 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-14 00:12:21,639 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-14 00:12:21,647 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-14 00:12:21,648 : INFO : training on 9090515 raw words (6065139 effective words) took 33.6s, 180610 effective words/s\n",
      "2017-06-14 00:12:21,649 : WARNING : supplied example count (644340) did not equal expected count (428900)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6065139"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harry_model.train(sentences, total_examples= harry_model.corpus_count, epochs= harry_model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-14 00:13:00,202 : INFO : saving Word2Vec object under ./trained_models/harry.w2v, separately None\n",
      "2017-06-14 00:13:00,205 : INFO : not storing attribute syn0norm\n",
      "2017-06-14 00:13:00,207 : INFO : not storing attribute cum_table\n",
      "2017-06-14 00:13:01,176 : INFO : saved ./trained_models/harry.w2v\n"
     ]
    }
   ],
   "source": [
    "harry_model.save(os.path.join(\"./trained_models/\", \"harry.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
